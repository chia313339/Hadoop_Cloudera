spark.default.parallelism	

For distributed shuffle operations like reduceByKey and join, 
the largest number of partitions in a parent RDD. 
For operations like parallelize with no parent RDDs, 
it depends on the cluster manager:
Local mode: number of cores on the local machine
Mesos fine grained mode: 8
Others: total number of cores on all executor nodes or 2, whichever is larger

Default number of partitions in RDDs returned by transformations 
like join, reduceByKey, and parallelize when not set by user.

In [5]: mydata = sc.parallelize(range(100),4)

In [7]: mydata.getNumPartitions()
Out[7]: 4

--應該盡量減少使用需要shuffle的transformation
	reductByKey
	join
	distinct
	repartition
	groupByKey
	aggregateByKey